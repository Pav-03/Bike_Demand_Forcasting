# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wrx5Y9cnrNklnQyk_3hUPKfVzZ4eGXu-

# Problem Statement

A client is exploring the feasibility of expanding their business into short-term rental cycles in London. To support their decision-making, they require insights into existing cycling usage patterns to understand demand, customer behavior, and operational challenges. Specifically, they are interested in:

1.   Customer segmentation
2.   Expected usage trends
2.   Operational concerns such as reliability and supply chain management


By analyzing Transport for London (TFL) cycling data (2021-2023), I aim to derive actionable insights that will help the client design an effective strategy for their expansion.

# Data Acquisition and Preprocessing Approach

## 1. Data Collection

Initially, an attempt was made to access the Transport for London (TFL) cycling data via API to streamline the data ingestion process. However, due to API inconsistencies and accessibility issues, the files were manually downloaded and organized year-wise to efficiently manage high-memory usage.

## 2. Data Quality Challenges

During the preliminary data inspection, several inconsistencies were identified:



*   Schema Variability: The 2022 dataset exhibited
discrepancies in column names, as the initial months followed a different naming convention compared to later months. These inconsistencies persisted in the 2023 data, requiring manual standardization.

*   Data Corruption: Certain records had misaligned values, where data points appeared under incorrect features. This issue was more pronounced in the 2022 dataset due to changes in data collection methodology.
*   Inconsistent Data Types: Multiple columns contained mixed data types (e.g., strings and numerical values within the same column), which could impact downstream data processing and model performance.


*   Missing Values: Several attributes contained null or missing values, requiring appropriate handling strategies.


## 3. Data Cleaning and Standardization

To ensure consistency across datasets, the following preprocessing steps were applied:

1.   Feature Name Standardization:
*   Column names were aligned with the 2021 dataset to maintain uniformity across all years.

2.   Data Type Optimization:
*   Converted numerical columns to appropriate integer/float formats to reduce memory footprint.
*   Standardized categorical variables to ensure consistent encoding.

3.   Handling Mixed Data Types:
*   Columns with multiple data types were transformed into a single, uniform format to facilitate efficient analysis and model training.

4.   Missing Value Treatment:

*   Used imputation techniques where applicable.
*   Dropped non-essential records if missing data exceeded a reasonable threshold.


## 4. Data Storage and Accessibility

The cleaned and preprocessed datasets were stored in Google Drive, ensuring efficient access for further analysis while optimizing memory management. This structured storage approach allows for seamless integration with subsequent data science tasks, including exploratory analysis, modeling, and visualization.


This systematic approach ensures data integrity, consistency, and scalability, providing a strong foundation for insightful analysis and data-driven decision-making.

# 01 - TFL Data Pipeline
"""

import os
import pandas as pd
import glob
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define folder paths
RAW_DATA_FOLDER = "/content/drive/My Drive/PwC_Data"
PROCESSED_DATA_FOLDER = "/content/drive/My Drive/PwC_Data/processed_parquet"
os.makedirs(PROCESSED_DATA_FOLDER, exist_ok=True)

# Find all CSV files
file_list = [os.path.join(RAW_DATA_FOLDER, f) for f in os.listdir(RAW_DATA_FOLDER) if f.endswith('.csv')]
print(f"Found {len(file_list)} CSV files for processing.")

# Install fastparquet if not installed
try:
    import fastparquet
except ImportError:
    !pip install fastparquet
    import fastparquet

# Define chunk size
CHUNK_SIZE = 500_000

def process_and_convert_to_parquet(file_path):
    """ Reads a CSV file in chunks, optimizes data types, and saves it as Parquet. """
    print(f"Processing: {os.path.basename(file_path)}")

    chunk_iter = pd.read_csv(file_path, chunksize=CHUNK_SIZE, low_memory=False)

    for i, chunk in enumerate(chunk_iter):
        # Optimize Data Types
        for col in chunk.select_dtypes(include=['float64']).columns:
            chunk[col] = chunk[col].astype('float32')

        for col in chunk.select_dtypes(include=['int64']).columns:
            chunk[col] = chunk[col].astype('int32')

        # Save each chunk separately
        output_file = os.path.join(PROCESSED_DATA_FOLDER, f"{os.path.basename(file_path).replace('.csv', '')}_chunk_{i}.parquet")
        chunk.to_parquet(output_file, index=False, compression='snappy', engine='fastparquet')

        print(f"Chunk {i} saved: {output_file}")

# Process each CSV file
for file in file_list:
    process_and_convert_to_parquet(file)

print("All files processed and saved as Parquet.")

# Merge all Parquet files into a single dataset
def merge_parquet_files(output_folder):
    """ Merges all processed Parquet files into a single DataFrame. """
    parquet_files = glob.glob(os.path.join(output_folder, "*.parquet"))

    if not parquet_files:
        print("No Parquet files found for merging.")
        return None

    print(f"Merging {len(parquet_files)} Parquet files...")
    df_list = [pd.read_parquet(f) for f in parquet_files]
    merged_df = pd.concat(df_list, ignore_index=True)

    # Save merged dataset
    final_output_file = os.path.join(output_folder, "final_merged.parquet")
    merged_df.to_parquet(final_output_file, index=False, compression='snappy')

    print(f"Final merged dataset saved: {final_output_file}")
    return final_output_file

# Run merging function
final_parquet_path = merge_parquet_files(PROCESSED_DATA_FOLDER)

# Load the final dataset
if final_parquet_path:
    df = pd.read_parquet(final_parquet_path)
    print("Data successfully loaded into DataFrame.")

"""# Data Pre-Processing Pipeline"""

import pandas as pd
import numpy as np
import logging

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def load_data(parquet_file):
    """Load Parquet file into DataFrame"""
    logging.info(f"Loading data from {parquet_file}...")
    df = pd.read_parquet(parquet_file)
    logging.info(f"Data Loaded. Shape: {df.shape}")
    return df

def inspect_data(df):
    """Display basic data insights"""
    logging.info("Data Overview:")
    logging.info(df.info())
    logging.info(df.head())
    logging.info(f"Missing Values:\n{df.isnull().sum()}")

def handle_missing_values(df):
    """Handle missing values in the dataset"""
    missing_count = df["EndStation Id"].isnull().sum()
    total_rows = len(df)
    missing_percentage = (missing_count / total_rows) * 100

    logging.warning(f"Missing EndStation Id: {missing_count} rows ({missing_percentage:.2f}%)")

    # Drop missing 'EndStation Id' values
    df = df.dropna(subset=["EndStation Id"])
    logging.info(f"Missing values handled. New shape: {df.shape}")
    return df

def optimize_data_types(df):
    """Optimize data types for memory efficiency"""
    int_columns = ["EndStation Id", "Rental Id", "Duration", "Bike Id", "StartStation Id"]

    for col in int_columns:
        df[col] = df[col].astype("int16")  # Convert to int16 to reduce memory usage

    logging.info(f"Data types optimized for: {int_columns}")
    return df

def remove_duplicates(df):
    """Remove duplicate rows"""
    duplicate_count = df.duplicated().sum()

    if duplicate_count > 0:
        logging.warning(f"Found {duplicate_count} duplicate rows. Removing...")
        df = df.drop_duplicates()
        logging.info("Duplicates removed.")
    else:
        logging.info("No duplicate rows found.")

    return df

def add_temporal_features(df):
    """Extract useful temporal features from Start Date"""
    df["Start Date"] = pd.to_datetime(df["Start Date"])
    df["End Date"] = pd.to_datetime(df["End Date"])

    df["Start_Date"] = df["Start Date"].dt.date  # Keep only the date
    df["Hour"] = df["Start Date"].dt.hour  # Extract hour
    df["Year"] = df["Start Date"].dt.year # Extract year
    df["Month"] = df["Start Date"].dt.month # Extract month
    df["Day"] = df["Start Date"].dt.day # Extract day
    df["Weekday"] = df["Start Date"].dt.weekday  # 0 = Monday, 6 = Sunday

    logging.info("Temporal features added.")
    return df

def calculate_bike_demand(df):
    """Calculate bike demand per station and date"""
    bike_demand = df.groupby(["StartStation Id", "Start_Date"])["Rental Id"].count().reset_index()
    bike_demand.rename(columns={"Rental Id": "Bike Demand"}, inplace=True)

    df = df.merge(bike_demand, on=["StartStation Id", "Start_Date"], how="left")

    logging.info("Bike demand calculated and merged.")
    return df

# Main Execution Flow
if __name__ == "__main__":
    FILE_PATH = "/content/drive/My Drive/PwC_Data/processed_parquet/final_merged.parquet"

    df = load_data(FILE_PATH)
    inspect_data(df)
    df = handle_missing_values(df)
    df = optimize_data_types(df)
    df = remove_duplicates(df)
    df = add_temporal_features(df)
    df = calculate_bike_demand(df)

    # Save Processed Data
    processed_file = "/content/drive/My Drive/PwC_Data/processed_parquet/processed_data.parquet"
    df.to_parquet(processed_file, index=False, compression='snappy')
    logging.info(f"Final processed dataset saved: {processed_file}")

"""# Exploratory Data Analysis Pipeline"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Set Seaborn style for better visual appeal
sns.set_style("whitegrid")

def plot_bike_demand_over_time(df):
    """Visualizes daily bike demand trends over time."""
    bike_demand = df.groupby("Start_Date")["Bike Demand"].sum().reset_index()
    bike_demand["Start_Date"] = pd.to_datetime(bike_demand["Start_Date"])
    bike_demand = bike_demand.sort_values("Start_Date")

    plt.figure(figsize=(12, 6))
    sns.lineplot(data=bike_demand, x="Start_Date", y="Bike Demand", marker="o", color="b")
    plt.xlabel("Date")
    plt.ylabel("Total Bike Demand")
    plt.title("Daily Bike Demand Over Time")
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.show()

def plot_hourly_rental_trends(df):
    """Visualizes hourly bike rental trends."""
    hourly_rentals = df.groupby("Hour")["Rental Id"].count().reset_index()

    plt.figure(figsize=(12, 6))
    sns.barplot(data=hourly_rentals, x="Hour", y="Rental Id", palette="viridis")
    plt.xlabel("Hour of the Day")
    plt.ylabel("Total Rentals")
    plt.title("Hourly Rental Trends")
    plt.grid(axis="y")
    plt.show()

def plot_top_start_stations(df, top_n=10):
    """Visualizes the most frequently used start stations."""
    top_stations = df["StartStation Name"].value_counts().nlargest(top_n)

    plt.figure(figsize=(12, 6))
    sns.barplot(y=top_stations.index, x=top_stations.values, palette="magma")
    plt.xlabel("Total Rentals")
    plt.ylabel("Start Station Name")
    plt.title("Most Popular Start Stations")
    plt.grid(axis="x")
    plt.show()

def plot_top_end_stations(df, top_n=10):
    """Visualizes the most frequently used end stations."""
    top_stations = df["EndStation Name"].value_counts().nlargest(top_n)

    plt.figure(figsize=(12, 6))
    sns.barplot(y=top_stations.index, x=top_stations.values, palette="magma")
    plt.xlabel("Total Rentals")
    plt.ylabel("End Station Name")
    plt.title("Most Popular End Stations")
    plt.grid(axis="x")
    plt.show()

def plot_hourly_heatmap(df):
    """Displays a heatmap of bike demand by hour and weekday."""
    df["Weekday"] = df["Start Date"].dt.day_name()
    heatmap_data = df.pivot_table(index="Weekday", columns="Hour", values="Rental Id", aggfunc="count")

    plt.figure(figsize=(12, 6))
    sns.heatmap(heatmap_data, cmap="coolwarm", annot=False, linewidths=0.5)
    plt.xlabel("Hour of the Day")
    plt.ylabel("Day of the Week")
    plt.title("Hourly Demand Trend")
    plt.show()

def plot_weekday_vs_weekend(df):
    """Compares bike rentals on weekdays vs. weekends."""
    df["Is_Weekend"] = df["Weekday"].isin(["Saturday", "Sunday"])
    weekend_rentals = df.groupby("Is_Weekend")["Rental Id"].count()

    plt.figure(figsize=(6, 6))
    sns.barplot(x=weekend_rentals.index, y=weekend_rentals.values, palette=["blue", "red"])
    plt.xticks(ticks=[0, 1], labels=["Weekday", "Weekend"])
    plt.ylabel("Total Rentals")
    plt.title("Weekday vs. Weekend Rentals")
    plt.grid(axis="y")
    plt.show()

def plot_seasonal_trends(df):
    """Analyzes bike rental trends across seasons."""
    season_map = {
        12: "Winter", 1: "Winter", 2: "Winter",
        3: "Spring", 4: "Spring", 5: "Spring",
        6: "Summer", 7: "Summer", 8: "Summer",
        9: "Autumn", 10: "Autumn", 11: "Autumn"
    }
    df["Season"] = df["Month"].map(season_map)
    season_rentals = df.groupby("Season")["Rental Id"].count().reindex(["Winter", "Spring", "Summer", "Autumn"])

    plt.figure(figsize=(8, 6))
    sns.barplot(x=season_rentals.index, y=season_rentals.values, palette="coolwarm")
    plt.xlabel("Season")
    plt.ylabel("Total Rentals")
    plt.title("Seasonal Trends in Bike Rentals")
    plt.grid(axis="y")
    plt.show()

def plot_trip_duration_distribution(df):
    """Visualizes the distribution of trip durations."""
    plt.figure(figsize=(12, 6))
    sns.histplot(df["Duration"], bins=50, kde=True, color="purple")
    plt.xlabel("Trip Duration (Seconds)")
    plt.ylabel("Frequency")
    plt.title("Distribution of Trip Durations")
    plt.xlim(0, df["Duration"].quantile(0.99))  # Limit outliers
    plt.grid(True)
    plt.show()

def plot_rush_hour_analysis(df):
    """Analyzes bike demand during peak rush hours."""
    rush_hour_df = df[df['Hour'].isin([6, 7, 8, 9, 16, 17, 18, 19])]
    station_demand = rush_hour_df.groupby("StartStation Name")["Bike Demand"].sum().reset_index()
    top_10_stations = station_demand.sort_values(by="Bike Demand", ascending=False).head(10)

    plt.figure(figsize=(12, 6))
    sns.barplot(x="StartStation Name", y="Bike Demand", data=top_10_stations, palette="Blues_d")
    plt.xlabel("Start Station Name")
    plt.ylabel("Total Bike Demand")
    plt.title("Top 10 Stations with Highest Demand During Rush Hours")
    plt.xticks(rotation=45)
    plt.show()

def plot_most_popular_routes(df):
    """Analyzes the most frequently used bike routes."""
    df["Route"] = df["StartStation Name"] + " ‚Üí " + df["EndStation Name"]
    top_routes = df["Route"].value_counts().nlargest(10)

    plt.figure(figsize=(12, 6))
    sns.barplot(y=top_routes.index, x=top_routes.values, palette="magma")
    plt.xlabel("Total Trips")
    plt.ylabel("Route")
    plt.title("Most Popular Routes")
    plt.grid(axis="x")
    plt.show()

# Main execution
if __name__ == "__main__":
    FILE_PATH = "/content/drive/My Drive/PwC_Data/processed_parquet/processed_data.parquet"
    df = pd.read_parquet(FILE_PATH)

    # Run all EDA functions
    plot_bike_demand_over_time(df)
    plot_hourly_rental_trends(df)
    plot_top_start_stations(df)
    plot_top_end_stations(df)
    plot_hourly_heatmap(df)
    plot_weekday_vs_weekend(df)
    plot_seasonal_trends(df)
    plot_trip_duration_distribution(df)
    plot_rush_hour_analysis(df)
    plot_most_popular_routes(df)

"""# Feature Engineering Pipeline"""

import pandas as pd
import numpy as np
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def drop_unnecessary_columns(df):
    """Removes non-essential columns that do not contribute to modeling."""
    cols_to_drop = ["Rental Id", "Bike Id", "End Date", "EndStation Id",
                    "EndStation Name", "StartStation Name", "Start_Date", "Route"]
    df = df.drop(columns=cols_to_drop, errors="ignore")
    logging.info(f"Dropped unnecessary columns: {cols_to_drop}")
    return df

def encode_categorical_features(df):
    """Encodes categorical variables into numerical format for machine learning models."""

    # Map weekdays to numerical values (Monday = 0, ..., Sunday = 6)
    weekday_map = {
        "Monday": 0, "Tuesday": 1, "Wednesday": 2, "Thursday": 3,
        "Friday": 4, "Saturday": 5, "Sunday": 6
    }
    df["Weekday"] = df["Weekday"].map(weekday_map)

    # Convert 'Is_Weekend' boolean flag to integer (0 = Weekday, 1 = Weekend)
    df["Is_Weekend"] = df["Is_Weekend"].astype(int)

    # Convert 'Season' into numerical encoding (Winter=0, Spring=1, Summer=2, Autumn=3)
    season_map = {"Winter": 0, "Spring": 1, "Summer": 2, "Autumn": 3}
    df["Season"] = df["Season"].map(season_map)

    logging.info("Categorical features encoded successfully.")
    return df

def create_lag_features(df):
    """Creates lagged features for time-series forecasting."""

    # Lag features for past bike demand
    df["Bike Demand Lag 1"] = df.groupby("StartStation Id")["Bike Demand"].shift(1)
    df["Bike Demand Lag 3"] = df.groupby("StartStation Id")["Bike Demand"].shift(3)
    df["Bike Demand Lag 7"] = df.groupby("StartStation Id")["Bike Demand"].shift(7)
    df["Bike Demand Lag 30"] = df.groupby("StartStation Id")["Bike Demand"].shift(30)

    # Fill missing values in lag features with station-wise mean
    for col in ["Bike Demand Lag 1", "Bike Demand Lag 3", "Bike Demand Lag 7", "Bike Demand Lag 30"]:
        df[col] = df.groupby("StartStation Id")[col].transform(lambda x: x.fillna(x.mean()))

    logging.info("Lag features created (including Lag 3) and missing values handled.")
    return df

def create_rolling_features(df):
    """Creates rolling average features to capture trends over time."""

    df["Rolling Avg 7"] = df.groupby("StartStation Id")["Bike Demand"].transform(lambda x: x.rolling(7, min_periods=1).mean())
    df["Rolling Avg 14"] = df.groupby("StartStation Id")["Bike Demand"].transform(lambda x: x.rolling(14, min_periods=1).mean())
    df["Rolling Avg 30"] = df.groupby("StartStation Id")["Bike Demand"].transform(lambda x: x.rolling(30, min_periods=1).mean())

    logging.info("Rolling average features created (including Rolling Avg 14).")
    return df

def create_interaction_features(df):
    """Creates new interaction-based features to capture demand patterns."""

    # Convert trip duration from seconds to minutes
    df["Duration_Minutes"] = df["Duration"] / 60

    # Convert date information into numerical representation
    df["DayOfYear"] = df["Month"] * 30 + df["Day"]

    # Create new derived features
    df["Bike Demand per Minute"] = df["Bike Demand"] / df["Duration_Minutes"]
    df["Weekend Effect"] = df["Is_Weekend"] * df["Bike Demand"]
    df["Seasonal Effect"] = df["Season"] * df["Bike Demand"]

    logging.info("Interaction features created successfully.")
    return df

def create_time_based_features(df):
    """Creates additional time-based features for trend analysis."""

    # Identify rush hour periods (Morning: 7-9 AM, Evening: 5-7 PM)
    df["Is_Rush_Hour"] = df["Hour"].apply(lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 19) else 0)

    logging.info("Time-based features added.")
    return df

def finalize_features(df):
    """Drops remaining NaN values after feature creation."""

    df.dropna(inplace=True)
    logging.info("Final dataset cleaned, NaN values dropped.")
    return df

# Main Execution
if __name__ == "__main__":
    FILE_PATH = "/content/drive/My Drive/PwC_Data/processed_parquet/processed_data.parquet"
    df = pd.read_parquet(FILE_PATH)

    df = drop_unnecessary_columns(df)
    df = encode_categorical_features(df)
    df = create_lag_features(df)
    df = create_rolling_features(df)
    df = create_interaction_features(df)
    df = create_time_based_features(df)
    df = finalize_features(df)

    # Save the processed dataset
    final_output_file = "/content/drive/My Drive/PwC_Data/processed_parquet/feature_engineered_data.parquet"
    df.to_parquet(final_output_file, index=False, compression="snappy")

    logging.info(f"Final feature-engineered dataset saved: {final_output_file}")

"""# Final Data Preparation for Model Building Pipeline"""

import pandas as pd
import numpy as np
import logging
from sklearn.preprocessing import MinMaxScaler

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Define feature set and target variable
FEATURES = [
    "Duration_Minutes", "StartStation Id", "Hour", "Year", "Month", "Day", "Weekday",
    "Is_Weekend", "Season", "Bike Demand Lag 1", "Bike Demand Lag 7",
    "Bike Demand Lag 30", "Rolling Avg 7", "Rolling Avg 30", "DayOfYear",
    "Bike Demand per Minute", "Weekend Effect", "Seasonal Effect",
    "Is_Rush_Hour", "Bike Demand Lag 3", "Rolling Avg 14"
]
TARGET = "Bike Demand"

def load_data(file_path):
    """Loads the processed Parquet file."""
    logging.info(f"Loading dataset from {file_path}...")
    df = pd.read_parquet(file_path)
    logging.info(f"Data loaded successfully. Shape: {df.shape}")
    return df

def handle_missing_values(df):
    """Handles missing values by filling with mean or dropping if necessary."""
    missing_values = df[FEATURES].isnull().sum()
    logging.info(f"Missing Values Before Processing:\n{missing_values[missing_values > 0]}")

    # Drop rows with missing target values
    df = df.dropna(subset=[TARGET])

    # Replace infinite values with NaN
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Fill missing values in float columns with column mean
    float_cols = df.select_dtypes(include=['float64']).columns
    df[float_cols] = df[float_cols].fillna(df[float_cols].mean())

    # Fill missing values in integer columns with column mean and convert to int16
    int_cols = df.select_dtypes(include=['int64']).columns
    df[int_cols] = df[int_cols].fillna(df[int_cols].mean()).astype('int16')

    missing_after = df[FEATURES].isnull().sum()
    logging.info(f"Missing Values After Processing:\n{missing_after[missing_after > 0]}")

    return df

def optimize_data_types(df):
    """Converts data types for memory efficiency."""
    float_cols = df.select_dtypes(include=['float64']).columns
    df[float_cols] = df[float_cols].astype("float32")

    int_cols = df.select_dtypes(include=['int64']).columns
    df[int_cols] = df[int_cols].astype("int16")

    logging.info(f"Data types optimized for memory efficiency.")
    return df

def normalize_features(df):
    """Normalizes continuous features using MinMaxScaler."""
    scaler = MinMaxScaler()
    df[FEATURES] = scaler.fit_transform(df[FEATURES])
    logging.info("Continuous features normalized using MinMaxScaler.")
    return df, scaler

# Main Execution
if __name__ == "__main__":
    FILE_PATH = "/content/drive/My Drive/PwC_Data/processed_parquet/feature_engineered_data.parquet"
    df = load_data(FILE_PATH)

    df = handle_missing_values(df)
    df = optimize_data_types(df)
    df, scaler = normalize_features(df)

    # Save the prepared dataset
    final_output_file = "/content/drive/My Drive/PwC_Data/processed_parquet/model_ready_data.parquet"
    df.to_parquet(final_output_file, index=False, compression="snappy")

    logging.info(f"Final model-ready dataset saved: {final_output_file}")

"""# Model Building Pipeline"""

import pandas as pd
import numpy as np
import logging
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Define features and target variable
FEATURES = [
    "Duration_Minutes", "StartStation Id", "Hour", "Year", "Month", "Day", "Weekday",
    "Is_Weekend", "Season", "Bike Demand Lag 1", "Bike Demand Lag 7",
    "Bike Demand Lag 30", "Rolling Avg 7", "Rolling Avg 30", "DayOfYear",
    "Bike Demand per Minute", "Weekend Effect", "Seasonal Effect",
    "Is_Rush_Hour", "Bike Demand Lag 3", "Rolling Avg 14"
]
TARGET = "Bike Demand"

def load_prepared_data(file_path):
    """Loads the model-ready dataset from a Parquet file."""
    logging.info(f"üìÇ Loading prepared dataset from {file_path}...")
    df = pd.read_parquet(file_path)
    logging.info(f"Data loaded. Shape: {df.shape}")
    return df

def split_data(df):
    """Splits the dataset into train and test sets."""
    X = df[FEATURES]
    y = df[TARGET]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    logging.info(f"Data split into training ({X_train.shape}) and testing ({X_test.shape}).")
    return X_train, X_test, y_train, y_test

def train_xgboost(X_train, X_test, y_train, y_test):
    """Trains an XGBoost regression model and evaluates it."""

    # Define model
    xgb_model = XGBRegressor(
        objective="reg:squarederror",
        n_estimators=1000,
        learning_rate=0.1,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="mae",
        early_stopping_rounds=10,
        tree_method="gpu_hist"  # Used GPU for fast processing
    )

    # Train the model
    logging.info("Training XGBoost model...")
    xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)

    logging.info("XGBoost model training completed.")
    return xgb_model

def evaluate_model(model, X_test, y_test):
    """Evaluates the trained model and prints performance metrics."""
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = r2_score(y_test, y_pred)

    logging.info(f"Model Evaluation:")
    logging.info(f"MAE: {mae:.2f}")
    logging.info(f"RMSE: {rmse:.2f}")
    logging.info(f"R¬≤ Score: {r2:.2f}")

    return mae, rmse, r2

# Main Execution
if __name__ == "__main__":
    FILE_PATH = "/content/drive/My Drive/PwC_Data/processed_parquet/model_ready_data.parquet"
    df = load_prepared_data(FILE_PATH)

    X_train, X_test, y_train, y_test = split_data(df)

    xgb_model = train_xgboost(X_train, X_test, y_train, y_test)

    mae, rmse, r2 = evaluate_model(xgb_model, X_test, y_test)

    logging.info("Model pipeline execution completed.")

"""# Model Evaluation Pipeline"""

import numpy as np
import logging
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def evaluate_model(model, X_train, y_train, X_test, y_test):
    """Evaluates model performance using MAE, RMSE, and R¬≤ score."""

    # Predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Compute evaluation metrics
    train_mae = mean_absolute_error(y_train, y_train_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)
    test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)

    logging.info(f"Model Performance Metrics:")
    logging.info(f"Train MAE: {train_mae:.2f} | Test MAE: {test_mae:.2f}")
    logging.info(f"Train RMSE: {train_rmse:.2f} | Test RMSE: {test_rmse:.2f}")
    logging.info(f"Train R¬≤ Score: {train_r2:.2f} | Test R¬≤ Score: {test_r2:.2f}")

    return {
        "train_mae": train_mae, "test_mae": test_mae,
        "train_rmse": train_rmse, "test_rmse": test_rmse,
        "train_r2": train_r2, "test_r2": test_r2
    }

def plot_feature_importance(model):
    """Plots the top 10 most important features in the trained model."""
    plt.figure(figsize=(10, 6))
    xgb.plot_importance(model, max_num_features=10, importance_type="gain", xlabel="Feature Importance")
    plt.title("üîç Top 10 Feature Importances in XGBoost Model")
    plt.show()
    logging.info("Feature importance plotted.")

# Main Execution
if __name__ == "__main__":
    # Assuming X_train, X_test, y_train, y_test, and xgb_model are already defined
    metrics = evaluate_model(xgb_model, X_train, y_train, X_test, y_test)
    plot_feature_importance(xgb_model)

    logging.info("Model evaluation pipeline completed.")

"""# Hyperpatameter Tunning with Best Model and Evaluation Pipeline"""

import optuna
import pandas as pd
import numpy as np
import logging
import matplotlib.pyplot as plt
import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def load_data(file_path):
    """Loads the model-ready dataset."""
    logging.info(f"Loading dataset from {file_path}...")
    df = pd.read_parquet(file_path)
    logging.info(f"Data loaded successfully. Shape: {df.shape}")
    return df

def split_data(df, features, target):
    """Splits the dataset into training and testing sets."""
    X_train, X_test, y_train, y_test = train_test_split(
        df[features], df[target], test_size=0.2, random_state=42
    )
    logging.info(f"Data split into training ({X_train.shape}) and testing ({X_test.shape}).")
    return X_train, X_test, y_train, y_test

def objective(trial, X_train, y_train):
    """Optimization function for tuning XGBoost hyperparameters with Optuna."""

    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 10),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),
        'tree_method': 'gpu_hist',  # Enables GPU acceleration
        'predictor': 'gpu_predictor'
    }

    # Perform cross-validation to evaluate performance
    model = XGBRegressor(**params)
    mae = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=3).mean()

    return -mae  # Optuna minimizes the function

def run_hyperparameter_optimization(X_train, y_train, n_trials=30):
    """Runs Optuna hyperparameter tuning and returns the best parameters."""
    logging.info("Running hyperparameter tuning with Optuna...")

    study = optuna.create_study(direction='minimize')
    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=n_trials)

    best_params = study.best_params
    logging.info(f"Best Hyperparameters: {best_params}")
    return best_params

def train_best_xgboost(X_train, X_test, y_train, y_test, best_params):
    """Trains an XGBoost model using the best hyperparameters."""
    logging.info("Training XGBoost model with optimized parameters...")

    best_xgb_model = XGBRegressor(**best_params)
    best_xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)

    logging.info("Model training completed.")
    return best_xgb_model

def evaluate_model(model, X_train, y_train, X_test, y_test):
    """Evaluates the optimized model and logs key metrics."""
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    train_mae = mean_absolute_error(y_train, y_train_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)

    logging.info(f"Model Performance:")
    logging.info(f"Train MAE: {train_mae:.2f} | Test MAE: {test_mae:.2f}")

    return train_mae, test_mae

def plot_feature_importance(model):
    """Plots the top 10 most important features in the optimized XGBoost model."""
    plt.figure(figsize=(10, 6))
    xgb.plot_importance(model, importance_type="weight", max_num_features=10)
    plt.title("üîç Top 10 Feature Importances in Optimized XGBoost Model")
    plt.show()
    logging.info("Feature importance plotted.")

# Main Execution
if __name__ == "__main__":
    FILE_PATH = "/content/drive/My Drive/PwC_Data/processed_parquet/model_ready_data.parquet"

    FEATURES = [
        "Duration_Minutes", "StartStation Id", "Hour", "Year", "Month", "Day", "Weekday",
        "Is_Weekend", "Season", "Bike Demand Lag 1", "Bike Demand Lag 7",
        "Bike Demand Lag 30", "Rolling Avg 7", "Rolling Avg 30", "DayOfYear",
        "Bike Demand per Minute", "Weekend Effect", "Seasonal Effect",
        "Is_Rush_Hour", "Bike Demand Lag 3", "Rolling Avg 14"
    ]
    TARGET = "Bike Demand"

    df = load_data(FILE_PATH)
    X_train, X_test, y_train, y_test = split_data(df, FEATURES, TARGET)

    best_params = run_hyperparameter_optimization(X_train, y_train, n_trials=30)

    best_xgb_model = train_best_xgboost(X_train, X_test, y_train, y_test, best_params)

    train_mae, test_mae = evaluate_model(best_xgb_model, X_train, y_train, X_test, y_test)

    plot_feature_importance(best_xgb_model)

    logging.info("Hyperparameter tuning pipeline completed.")